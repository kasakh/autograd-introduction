{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Backprop-Examples\">Backprop Examples<a class=\"anchor-link\" href=\"#Backprop-Examples\">¶</a></h1><p>We will look at three different ways of computing gradients, all of which rely on the same basic principle (backprop).</p>\n",
    "<ol>\n",
    "<li>The first method is to code a function and its derivatives manually using the rules of backprop.</li>\n",
    "<li>The second method is to use automatic differentiation using a tool called <a href=\"https://github.com/HIPS/autograd\">autograd</a>.</li>\n",
    "<li>The third method is to use <a href=\"https://www.tensorflow.org/\">Tensorflow</a>, a powerful machine learning toolkit from Google, which also includes methods to automatically differentiate an expression defined through Tensforflow primitives. </li>\n",
    "</ol>\n",
    "<h3 id=\"More-about-autograd\">More about <code>autograd</code><a class=\"anchor-link\" href=\"#More-about-autograd\">¶</a></h3><p><a href=\"https://github.com/HIPS/autograd\">autograd</a> is a Python package for <strong>algorithmic differentiation</strong>. It allows you to automatically compute the derivative of functions written in (nearly) native code. This makes it really easy to compute derivatives. Under the hood, it is also using reverse mode autodiff (backprop).</p>\n",
    "<h3 id=\"Mote-about-Tensorflow\">Mote about Tensorflow<a class=\"anchor-link\" href=\"#Mote-about-Tensorflow\">¶</a></h3><p><a href=\"https://www.tensorflow.org/\">Tensorflow</a> is a powerful machine learning package from Google.</p>\n",
    "<p>You can define functions as \"computation graphs\" using tensor flow operations, and it can automatically perform backprop (aka reverse mode automatic differentiation) to compute the gradient for you.</p>\n",
    "<p><strong>Note:</strong> You will need to install TensorFlow if you want to run these examples. It is not hard to install, but takes a bit more work to be able to call from a Jupyter notebook.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Backprop-Example-1\">Backprop Example 1<a class=\"anchor-link\" href=\"#Backprop-Example-1\">¶</a></h1><p>We will see three different ways to compute the gradient of</p>\n",
    "$$f(x,y,z) = (2x + y)z$$<h3 id=\"Manual-backprop\">Manual backprop<a class=\"anchor-link\" href=\"#Manual-backprop\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Backprop example\n",
    "\n",
    "# Compute f(x,y,z) = (2*x+y)*z\n",
    "x = 1.\n",
    "y = 2.\n",
    "z = 3.\n",
    "\n",
    "# Forward pass\n",
    "q = 2.*x + y   # Node 1\n",
    "f = q*z        # Node 2\n",
    "\n",
    "# Backward pass\n",
    "df_dq = z          # Node 2 input\n",
    "df_dz = q          # Node 2 input\n",
    "df_dx = 2 * df_dq  # Node 1 input\n",
    "df_dy = 1 * df_dq  # Node 1 input\n",
    "\n",
    "grad = np.array([df_dx, df_dy, df_dz])\n",
    "\n",
    "print f\n",
    "print grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"Autograd\">Autograd<a class=\"anchor-link\" href=\"#Autograd\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import autograd.numpy as np  # Thinly wrapped version of numpy\n",
    "from autograd import grad\n",
    "\n",
    "def f(args):\n",
    "    x,y,z = args\n",
    "    return (2*x + y)*z\n",
    "\n",
    "f_grad = grad(f)  # magic: returns a function that computes the gradient of f\n",
    "\n",
    "x = 1.\n",
    "y = 2.\n",
    "z = 3.\n",
    "\n",
    "print f([x, y, z])\n",
    "print f_grad([x, y, z])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"Tensorflow\">Tensorflow<a class=\"anchor-link\" href=\"#Tensorflow\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define tensor flow variables x, y, z\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "# Define the function\n",
    "f = (2*x + y)*z    # NB: operator overloading. \n",
    "                   \n",
    "# The output f is a tensforflow object representing the computation \n",
    "# graph to compute f\n",
    "\n",
    "# Define gradient object\n",
    "grad = tf.gradients(f, [x, y, z])\n",
    "\n",
    "# Use session to compute the values for f and grad\n",
    "session = tf.Session()\n",
    "values = session.run([f, grad],                 # query these output values\n",
    "                     {x: [1], y: [2], z: [3]})  # specify input values for x, y, z\n",
    "session.close()\n",
    "\n",
    "# Print result as numpy arrays\n",
    "print np.array(values[0]) \n",
    "print np.array(values[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Backprop-Example-2\">Backprop Example 2<a class=\"anchor-link\" href=\"#Backprop-Example-2\">¶</a></h1><p>Here is a slighly more complex example:</p>\n",
    "$$f(x) = 10*\\exp(\\sin(x)) + \\cos^2(x)$$<h3 id=\"Manual-backprop\">Manual backprop<a class=\"anchor-link\" href=\"#Manual-backprop\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Backprop example\n",
    "# f(x) = 10*np.exp(np.sin(x)) + np.cos(x)**2\n",
    "\n",
    "# Forward pass\n",
    "x = 2\n",
    "a = np.sin(x)   # Node 1\n",
    "b = np.cos(x)   # Node 1\n",
    "c = b**2        # Node 3\n",
    "d = np.exp(a)   # Node 4\n",
    "f = 10*d + c    # Node 5 (final output)\n",
    "\n",
    "# Backward pass\n",
    "df_dd = 10                    # Node 5 input\n",
    "df_dc = 1                     # Node 5 input\n",
    "df_da = np.exp(a) * df_dd     # Node 4 input\n",
    "df_db = 2*b * df_dc           # Node 3 input\n",
    "df_dx =  np.cos(x) * df_da - np.sin(x) * df_db  # Node 2 and 1 input\n",
    "\n",
    "print f, df_dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"Autograd\">Autograd<a class=\"anchor-link\" href=\"#Autograd\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import autograd.numpy as np  # Thinly wrapped version of numpy\n",
    "from autograd import grad\n",
    "\n",
    "def f(x):\n",
    "    return 10*np.exp(np.sin(x)) + np.cos(x)**2\n",
    "\n",
    "f_grad = grad(f)\n",
    "\n",
    "x = 2.\n",
    "\n",
    "print f(x)\n",
    "print f_grad(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"Tensorflow\">Tensorflow<a class=\"anchor-link\" href=\"#Tensorflow\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "# Here is the expression for our function\n",
    "f = 10*tf.exp(tf.sin(x)) + tf.cos(x)**2\n",
    "\n",
    "df_dx = tf.gradients(f, x)\n",
    "\n",
    "session = tf.Session()\n",
    "values = session.run([f, df_dx], \n",
    "                     {x: [2]})\n",
    "session.close()\n",
    "\n",
    "print np.array(values[0])\n",
    "print np.array(values[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
